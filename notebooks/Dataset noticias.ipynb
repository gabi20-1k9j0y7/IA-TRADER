{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3261059",
   "metadata": {},
   "source": [
    "# 1. EXTRACCION DE NOTICIAS NYT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d0858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NYT Amazon COMPANY – Rango: 2020-07 .. 2025-07\n",
      "============================================================\n",
      "Procesando 2020-07 …\n",
      "  -> 2020-07: INCLUIDOS 17\n",
      "Procesando 2020-08 …\n",
      "  -> 2020-08: INCLUIDOS 11\n",
      "Procesando 2020-09 …\n",
      "  -> 2020-09: INCLUIDOS 11\n",
      "Procesando 2020-10 …\n",
      "  -> 2020-10: INCLUIDOS 17\n",
      "Procesando 2020-11 …\n",
      "  -> 2020-11: INCLUIDOS 10\n",
      "Procesando 2020-12 …\n",
      "  -> 2020-12: INCLUIDOS 7\n",
      "Procesando 2021-01 …\n",
      "  -> 2021-01: INCLUIDOS 17\n",
      "Procesando 2021-02 …\n",
      "  -> 2021-02: INCLUIDOS 23\n",
      "Procesando 2021-03 …\n",
      "  -> 2021-03: INCLUIDOS 32\n",
      "Procesando 2021-04 …\n",
      "  -> 2021-04: INCLUIDOS 28\n",
      "Procesando 2021-05 …\n",
      "  -> 2021-05: INCLUIDOS 19\n",
      "Procesando 2021-06 …\n",
      "  -> 2021-06: INCLUIDOS 20\n",
      "Procesando 2021-07 …\n",
      "  -> 2021-07: INCLUIDOS 19\n",
      "Procesando 2021-08 …\n",
      "  -> 2021-08: INCLUIDOS 10\n",
      "Procesando 2021-09 …\n",
      "  -> 2021-09: INCLUIDOS 14\n",
      "Procesando 2021-10 …\n",
      "  -> 2021-10: INCLUIDOS 12\n",
      "Procesando 2021-11 …\n",
      "  -> 2021-11: INCLUIDOS 17\n",
      "Procesando 2021-12 …\n",
      "  -> 2021-12: INCLUIDOS 11\n",
      "Procesando 2022-01 …\n",
      "  -> 2022-01: INCLUIDOS 10\n",
      "Procesando 2022-02 …\n",
      "  -> 2022-02: INCLUIDOS 10\n",
      "Procesando 2022-03 …\n",
      "  -> 2022-03: INCLUIDOS 13\n",
      "Procesando 2022-04 …\n",
      "  -> 2022-04: INCLUIDOS 33\n",
      "Procesando 2022-05 …\n",
      "  -> 2022-05: INCLUIDOS 10\n",
      "Procesando 2022-06 …\n",
      "  -> 2022-06: INCLUIDOS 15\n",
      "Procesando 2022-07 …\n",
      "  -> 2022-07: INCLUIDOS 16\n",
      "Procesando 2022-08 …\n",
      "  -> 2022-08: INCLUIDOS 6\n",
      "Procesando 2022-09 …\n",
      "  -> 2022-09: INCLUIDOS 6\n",
      "Procesando 2022-10 …\n",
      "  -> 2022-10: INCLUIDOS 9\n",
      "Procesando 2022-11 …\n",
      "  -> 2022-11: INCLUIDOS 13\n",
      "Procesando 2022-12 …\n",
      "  -> 2022-12: INCLUIDOS 10\n",
      "Procesando 2023-01 …\n",
      "  -> 2023-01: INCLUIDOS 11\n",
      "Procesando 2023-02 …\n",
      "  -> 2023-02: INCLUIDOS 3\n",
      "Procesando 2023-03 …\n",
      "  -> 2023-03: INCLUIDOS 6\n",
      "Procesando 2023-04 …\n",
      "  -> 2023-04: INCLUIDOS 4\n",
      "Procesando 2023-05 …\n",
      "  -> 2023-05: INCLUIDOS 4\n",
      "Procesando 2023-06 …\n",
      "  -> 2023-06: INCLUIDOS 6\n",
      "Procesando 2023-07 …\n",
      "  -> 2023-07: INCLUIDOS 10\n",
      "Procesando 2023-08 …\n",
      "  -> 2023-08: INCLUIDOS 8\n",
      "Procesando 2023-09 …\n",
      "  -> 2023-09: INCLUIDOS 13\n",
      "Procesando 2023-10 …\n",
      "  -> 2023-10: INCLUIDOS 5\n",
      "Procesando 2023-11 …\n",
      "  -> 2023-11: INCLUIDOS 7\n",
      "Procesando 2023-12 …\n",
      "  -> 2023-12: INCLUIDOS 5\n",
      "Procesando 2024-01 …\n",
      "  -> 2024-01: INCLUIDOS 10\n",
      "Procesando 2024-02 …\n",
      "  -> 2024-02: INCLUIDOS 6\n",
      "Procesando 2024-03 …\n",
      "  -> 2024-03: INCLUIDOS 3\n",
      "Procesando 2024-04 …\n",
      "  -> 2024-04: INCLUIDOS 2\n",
      "Procesando 2024-05 …\n",
      "  -> 2024-05: INCLUIDOS 2\n",
      "Procesando 2024-06 …\n",
      "  -> 2024-06: INCLUIDOS 5\n",
      "Procesando 2024-07 …\n",
      "  -> 2024-07: INCLUIDOS 8\n",
      "Procesando 2024-08 …\n",
      "  -> 2024-08: INCLUIDOS 4\n",
      "Procesando 2024-09 …\n",
      "  -> 2024-09: INCLUIDOS 7\n",
      "Procesando 2024-10 …\n",
      "  -> 2024-10: INCLUIDOS 5\n",
      "Procesando 2024-11 …\n",
      "  -> 2024-11: INCLUIDOS 7\n",
      "Procesando 2024-12 …\n",
      "  -> 2024-12: INCLUIDOS 14\n",
      "Procesando 2025-01 …\n",
      "  -> 2025-01: INCLUIDOS 7\n",
      "Procesando 2025-02 …\n",
      "  -> 2025-02: INCLUIDOS 6\n",
      "Procesando 2025-03 …\n",
      "  -> 2025-03: INCLUIDOS 3\n",
      "Procesando 2025-04 …\n",
      "  -> Error 2025-04: HTTPSConnectionPool(host='api.nytimes.com', port=443): Read timed out.. Reintentando en 8s…\n",
      "  -> 2025-04: INCLUIDOS 7\n",
      "Procesando 2025-05 …\n",
      "  -> 2025-05: INCLUIDOS 1\n",
      "Procesando 2025-06 …\n",
      "  -> Error 2025-06: 403 Client Error: Forbidden for url: https://api.nytimes.com/svc/archive/v1/2025/6.json?api-key=eUpVQ68I262NMSzMBlLlRV9YWRYOlnGy. Reintentando en 8s…\n",
      "  -> Error 2025-06: 403 Client Error: Forbidden for url: https://api.nytimes.com/svc/archive/v1/2025/6.json?api-key=eUpVQ68I262NMSzMBlLlRV9YWRYOlnGy. Reintentando en 16s…\n",
      "  -> Error 2025-06: 403 Client Error: Forbidden for url: https://api.nytimes.com/svc/archive/v1/2025/6.json?api-key=eUpVQ68I262NMSzMBlLlRV9YWRYOlnGy. Reintentando en 32s…\n",
      "  -> Error 2025-06: 403 Client Error: Forbidden for url: https://api.nytimes.com/svc/archive/v1/2025/6.json?api-key=eUpVQ68I262NMSzMBlLlRV9YWRYOlnGy. Reintentando en 64s…\n",
      "  -> ERROR FATAL en 2025-06: 403 Client Error: Forbidden for url: https://api.nytimes.com/svc/archive/v1/2025/6.json?api-key=eUpVQ68I262NMSzMBlLlRV9YWRYOlnGy\n",
      "Procesando 2025-07 …\n",
      "  -> Error 2025-07: 403 Client Error: Forbidden for url: https://api.nytimes.com/svc/archive/v1/2025/7.json?api-key=eUpVQ68I262NMSzMBlLlRV9YWRYOlnGy. Reintentando en 8s…\n",
      "  -> Error 2025-07: 403 Client Error: Forbidden for url: https://api.nytimes.com/svc/archive/v1/2025/7.json?api-key=eUpVQ68I262NMSzMBlLlRV9YWRYOlnGy. Reintentando en 16s…\n",
      "  -> Error 2025-07: 403 Client Error: Forbidden for url: https://api.nytimes.com/svc/archive/v1/2025/7.json?api-key=eUpVQ68I262NMSzMBlLlRV9YWRYOlnGy. Reintentando en 32s…\n",
      "  -> Error 2025-07: 403 Client Error: Forbidden for url: https://api.nytimes.com/svc/archive/v1/2025/7.json?api-key=eUpVQ68I262NMSzMBlLlRV9YWRYOlnGy. Reintentando en 64s…\n",
      "  -> ERROR FATAL en 2025-07: 403 Client Error: Forbidden for url: https://api.nytimes.com/svc/archive/v1/2025/7.json?api-key=eUpVQ68I262NMSzMBlLlRV9YWRYOlnGy\n",
      "\n",
      "============================================================\n",
      "TOTAL INCLUIDOS (clean): 625 -> FINAL_NYT_NEWS_5_ANIOS.CSV\n",
      "TOTAL EXCLUIDOS (audit): 242929 -> nyt_amazon_company_2020-07_to_2025-07_audit.csv\n",
      "------------------------------------------------------------\n",
      "Conteo mensual (INCLUIDOS):\n",
      "  2020-07: 17\n",
      "  2020-08: 11\n",
      "  2020-09: 11\n",
      "  2020-10: 17\n",
      "  2020-11: 10\n",
      "  2020-12: 7\n",
      "  2021-01: 17\n",
      "  2021-02: 23\n",
      "  2021-03: 32\n",
      "  2021-04: 28\n",
      "  2021-05: 19\n",
      "  2021-06: 20\n",
      "  2021-07: 19\n",
      "  2021-08: 10\n",
      "  2021-09: 14\n",
      "  2021-10: 12\n",
      "  2021-11: 17\n",
      "  2021-12: 11\n",
      "  2022-01: 10\n",
      "  2022-02: 10\n",
      "  2022-03: 13\n",
      "  2022-04: 33\n",
      "  2022-05: 10\n",
      "  2022-06: 15\n",
      "  2022-07: 16\n",
      "  2022-08: 6\n",
      "  2022-09: 6\n",
      "  2022-10: 9\n",
      "  2022-11: 13\n",
      "  2022-12: 10\n",
      "  2023-01: 11\n",
      "  2023-02: 3\n",
      "  2023-03: 6\n",
      "  2023-04: 4\n",
      "  2023-05: 4\n",
      "  2023-06: 6\n",
      "  2023-07: 10\n",
      "  2023-08: 8\n",
      "  2023-09: 13\n",
      "  2023-10: 5\n",
      "  2023-11: 7\n",
      "  2023-12: 5\n",
      "  2024-01: 10\n",
      "  2024-02: 6\n",
      "  2024-03: 3\n",
      "  2024-04: 2\n",
      "  2024-05: 2\n",
      "  2024-06: 5\n",
      "  2024-07: 8\n",
      "  2024-08: 4\n",
      "  2024-09: 7\n",
      "  2024-10: 5\n",
      "  2024-11: 7\n",
      "  2024-12: 14\n",
      "  2025-01: 7\n",
      "  2025-02: 6\n",
      "  2025-03: 3\n",
      "  2025-04: 7\n",
      "  2025-05: 1\n",
      "------------------------------------------------------------\n",
      "Top motivos de EXCLUSIÓN:\n",
      "why_excluded\n",
      "no_amazon_in_title_or_abstract             242657\n",
      "hard_excluded_section_or_desk(Arts)           122\n",
      "hard_excluded_section_or_desk(Podcasts)        23\n",
      "hard_excluded_section_or_desk(Movies)          21\n",
      "hard_excluded_section_or_desk(Books)           21\n",
      "amazon_rainforest_context                      19\n",
      "hard_excluded_type(Interactive Feature)        16\n",
      "hard_excluded_section_or_desk(Style)           12\n",
      "blue_origin_without_kuiper                     11\n",
      "hard_excluded_type(Briefing)                    9\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "NYT – Amazon COMPANY (2020-07..2025-07)\n",
    "- Incluye si 'amazon'/'aws'/'amzn' aparece en titular/resumen\n",
    "- Excluye secciones/desks de cultura/estilo y tipos no-noticia\n",
    "- Filtra 'selva Amazónica' salvo marcadores corporativos\n",
    "- Blue Origin solo si hay Kuiper / “Amazon satellite”\n",
    "- Muestra conteo mensual y guarda clean + audit\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# ===== Parámetros =====\n",
    "# Clave eliminada por seguridad para publicación en repositorio\n",
    "NYT_API_KEY = \"TU_API_KEY_AQUI\"  \n",
    "\n",
    "START_DATE = date(2020, 7, 1)   # 01-07-2020\n",
    "END_DATE   = date(2025, 7, 31)  # 31-07-2025\n",
    "\n",
    "REQUEST_DELAY_S = 8\n",
    "MAX_REINTENTOS = 5\n",
    "GUARDAR_POR_MES = False\n",
    "\n",
    "# Palabras clave en titular/resumen\n",
    "TITLE_ABSTRACT_REGEX = re.compile(r\"\\b(amazon|aws|amzn)\\b\", re.IGNORECASE)\n",
    "\n",
    "# Exclusiones por sección/desk\n",
    "HARD_EXCLUDE_SECTIONS = {\n",
    "    \"Arts\", \"Arts&Leisure\", \"Books\", \"Movies\", \"Style\", \"Theater\", \"Podcasts\"\n",
    "}\n",
    "HARD_EXCLUDE_DESKS = {\n",
    "    \"Arts\", \"Arts&Leisure\", \"Books\", \"Culture\", \"Podcasts\", \"Styles\", \"Theater\", \"Weekend\"\n",
    "}\n",
    "\n",
    "# Tipos a excluir\n",
    "HARD_EXCLUDE_TYPES = {\n",
    "    \"briefing\", \"interactive feature\", \"obituary\", \"obit\", \"quote\", \"review\", \"video\"\n",
    "}\n",
    "\n",
    "# Pistas de selva Amazónica\n",
    "AMAZON_RAINFOREST_HINTS = {\n",
    "    \"rainforest\", \"rain forest\", \"river\", \"río\", \"indigenous\", \"tribe\", \"tribes\", \"indígena\", \"indígenas\",\n",
    "    \"deforestation\", \"reforestation\", \"wildfire\", \"wildfires\", \"megafire\", \"megafires\",\n",
    "    \"drought\", \"droughts\", \"ecuador\", \"peru\", \"brazil\", \"amazonia\", \"amazonas\", \"colombia\",\n",
    "    \"fires\", \"heat\", \"water level\", \"low water\", \"dried up\"\n",
    "}\n",
    "\n",
    "# Marcadores corporativos\n",
    "CORPORATE_MARKERS = {\n",
    "    \"aws\", \"prime\", \"warehouse\", \"warehouses\", \"fulfillment\", \"delivery\", \"logistics\", \"retail\",\n",
    "    \"nlrb\", \"osha\", \"teamsters\", \"union\", \"labor\", \"strike\", \"workers\", \"workforce\", \"employment\",\n",
    "    \"marketplace\", \"merchant\", \"third-party sellers\", \"advertising\", \"cloud\", \"anthropic\", \"kuiper\",\n",
    "    \"satellite\", \"data center\", \"datacenter\", \"ai\", \"earnings\", \"revenue\", \"acquisition\", \"roomba\",\n",
    "    \"ring\", \"whole foods\", \"twitch\", \"zoox\"\n",
    "}\n",
    "\n",
    "# Blue Origin: permitido solo con Kuiper/satélite Amazon\n",
    "BLUE_ORIGIN_ALLOW_IF = {\"kuiper\", \"amazon satellite\", \"amazon’s satellite\", \"project kuiper\"}\n",
    "\n",
    "# ===== Helpers =====\n",
    "def _norm(s): return (s or \"\").strip()\n",
    "def _lc(s):   return (s or \"\").lower()\n",
    "\n",
    "def _type_of_material(doc):\n",
    "    t = _lc(doc.get(\"type_of_material\"))\n",
    "    if not t:\n",
    "        t = _lc(doc.get(\"document_type\"))\n",
    "    return t\n",
    "\n",
    "def _news_desk(doc):    return _norm(doc.get(\"news_desk\"))\n",
    "def _section_name(doc): return _norm(doc.get(\"section_name\"))\n",
    "def _headline(doc):     return _norm(doc.get(\"headline\", {}).get(\"main\"))\n",
    "def _abstract(doc):     return _norm(doc.get(\"abstract\"))\n",
    "def _lead_paragraph(doc): return _norm(doc.get(\"lead_paragraph\"))\n",
    "\n",
    "def _byline(doc):\n",
    "    by = doc.get(\"byline\") or {}\n",
    "    if isinstance(by, dict) and by.get(\"original\"):\n",
    "        return _norm(by.get(\"original\"))\n",
    "    return \"\"\n",
    "\n",
    "def matches_title_or_abstract(doc):\n",
    "    text = f\"{_headline(doc)} || {_abstract(doc)}\"\n",
    "    return bool(TITLE_ABSTRACT_REGEX.search(text))\n",
    "\n",
    "def hard_excluded_section_or_desk(doc):\n",
    "    sec, desk = _section_name(doc), _news_desk(doc)\n",
    "    if sec in HARD_EXCLUDE_SECTIONS:\n",
    "        return f\"hard_excluded_section_or_desk({sec})\"\n",
    "    if desk in HARD_EXCLUDE_DESKS:\n",
    "        return f\"hard_excluded_section_or_desk({desk})\"\n",
    "    return \"\"\n",
    "\n",
    "def hard_excluded_type(doc):\n",
    "    t = _type_of_material(doc)\n",
    "    if t in HARD_EXCLUDE_TYPES:\n",
    "        return f\"hard_excluded_type({t.title()})\"\n",
    "    return \"\"\n",
    "\n",
    "def is_blue_origin_allowed(text_lc: str) -> bool:\n",
    "    if \"blue origin\" in text_lc:\n",
    "        return any(k in text_lc for k in BLUE_ORIGIN_ALLOW_IF)\n",
    "    return True\n",
    "\n",
    "def rainforest_context_without_corp(text_lc: str) -> bool:\n",
    "    has_rainforest = any(h in text_lc for h in AMAZON_RAINFOREST_HINTS)\n",
    "    if not has_rainforest:\n",
    "        return False\n",
    "    has_corp = any(k in text_lc for k in CORPORATE_MARKERS)\n",
    "    return not has_corp\n",
    "\n",
    "def should_include(doc):\n",
    "    if not matches_title_or_abstract(doc):\n",
    "        return False, \"no_amazon_in_title_or_abstract\"\n",
    "    w = hard_excluded_section_or_desk(doc)\n",
    "    if w: return False, w\n",
    "    w = hard_excluded_type(doc)\n",
    "    if w: return False, w\n",
    "    text_lc = _lc(\" \".join([_headline(doc), _abstract(doc), _lead_paragraph(doc), _byline(doc)]))\n",
    "    if rainforest_context_without_corp(text_lc):\n",
    "        return False, \"amazon_rainforest_context\"\n",
    "    if not is_blue_origin_allowed(text_lc):\n",
    "        return False, \"blue_origin_without_kuiper\"\n",
    "    return True, \"\"\n",
    "\n",
    "def to_row(doc):\n",
    "    return {\n",
    "        \"Fecha\":   pd.to_datetime(doc.get(\"pub_date\"), errors=\"coerce\"),\n",
    "        \"Titular\": _headline(doc),\n",
    "        \"Resumen\": _abstract(doc),\n",
    "        \"URL\":     _norm(doc.get(\"web_url\")),\n",
    "        \"Seccion\": _section_name(doc),\n",
    "        \"Desk\":    _news_desk(doc),\n",
    "        \"Tipo\":    (_type_of_material(doc) or \"\").title(),\n",
    "        \"Autor\":   _byline(doc),\n",
    "    }\n",
    "\n",
    "# ===== Cliente NYT =====\n",
    "def fetch_archive(year: int, month: int, api_key: str, base_delay: int = REQUEST_DELAY_S):\n",
    "    url = f\"https://api.nytimes.com/svc/archive/v1/{year}/{month}.json\"\n",
    "    params = {\"api-key\": api_key}\n",
    "    tries = 0\n",
    "    while True:\n",
    "        tries += 1\n",
    "        try:\n",
    "            time.sleep(base_delay)\n",
    "            r = requests.get(url, params=params, timeout=60)\n",
    "            if r.status_code == 429:\n",
    "                wait = min(base_delay * (2 ** (tries - 1)), 120)\n",
    "                print(f\"  -> 429 Too Many Requests. Backoff {wait}s (intento {tries}/{MAX_REINTENTOS})\")\n",
    "                time.sleep(wait)\n",
    "                if tries >= MAX_REINTENTOS:\n",
    "                    r.raise_for_status()\n",
    "                continue\n",
    "            r.raise_for_status()\n",
    "            return r.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if tries >= MAX_REINTENTOS:\n",
    "                raise\n",
    "            wait = min(base_delay * (2 ** (tries - 1)), 120)\n",
    "            print(f\"  -> Error {year}-{month:02d}: {e}. Reintentando en {wait}s…\")\n",
    "            time.sleep(wait)\n",
    "\n",
    "# ===== Iteración mensual =====\n",
    "def iter_months(start_date: date, end_date: date):\n",
    "    cur = date(start_date.year, start_date.month, 1)\n",
    "    endm = date(end_date.year, end_date.month, 1)\n",
    "    while cur <= endm:\n",
    "        yield cur.year, cur.month\n",
    "        cur = cur + relativedelta(months=1)\n",
    "\n",
    "# ===== Pipeline =====\n",
    "def main():\n",
    "    assert NYT_API_KEY and NYT_API_KEY != \"TU_API_KEY_AQUI\", \"Falta NYT_API_KEY\"\n",
    "\n",
    "    start_month_label = f\"{START_DATE.year}-{START_DATE.month:02d}\"\n",
    "    end_month_label   = f\"{END_DATE.year}-{END_DATE.month:02d}\"\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(f\"NYT Amazon COMPANY – Rango: {start_month_label} .. {end_month_label}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    incluidos, audit, monthly_counts = [], [], {}\n",
    "\n",
    "    for (y, m) in iter_months(START_DATE, END_DATE):\n",
    "        etiqueta = f\"{y}-{m:02d}\"\n",
    "        print(f\"Procesando {etiqueta} …\")\n",
    "        try:\n",
    "            data = fetch_archive(y, m, NYT_API_KEY)\n",
    "        except Exception as e:\n",
    "            print(f\"  -> ERROR FATAL en {etiqueta}: {e}\")\n",
    "            continue\n",
    "\n",
    "        docs = data.get(\"response\", {}).get(\"docs\", [])\n",
    "        mes_incluidos = 0\n",
    "\n",
    "        for doc in docs:\n",
    "            row = to_row(doc)\n",
    "            inc, why = should_include(doc)\n",
    "            if inc:\n",
    "                incluidos.append(row)\n",
    "                mes_incluidos += 1\n",
    "            else:\n",
    "                ar = dict(row)\n",
    "                ar[\"why_excluded\"] = why\n",
    "                audit.append(ar)\n",
    "\n",
    "        monthly_counts[(y, m)] = mes_incluidos\n",
    "        print(f\"  -> {etiqueta}: INCLUIDOS {mes_incluidos}\")\n",
    "\n",
    "        if GUARDAR_POR_MES:\n",
    "            if mes_incluidos:\n",
    "                dfm = pd.DataFrame([r for r in incluidos if r[\"Fecha\"].year == y and r[\"Fecha\"].month == m])\n",
    "                if not dfm.empty:\n",
    "                    outm = f\"nyt_amazon_company_{y}_{m:02d}_clean.csv\"\n",
    "                    dfm.sort_values(\"Fecha\", ascending=False).to_csv(outm, index=False, encoding=\"utf-8-sig\")\n",
    "            dfa = pd.DataFrame([r for r in audit if isinstance(r.get(\"Fecha\"), pd.Timestamp)\n",
    "                                and r[\"Fecha\"].year == y and r[\"Fecha\"].month == m])\n",
    "            if not dfa.empty:\n",
    "                outa = f\"nyt_amazon_company_{y}_{m:02d}_audit.csv\"\n",
    "                dfa.sort_values(\"Fecha\", ascending=False).to_csv(outa, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Guardado final\n",
    "    df_clean = pd.DataFrame(incluidos)\n",
    "    if not df_clean.empty:\n",
    "        df_clean = df_clean.dropna(subset=[\"Fecha\"]).drop_duplicates(subset=[\"URL\"]).sort_values(\"Fecha\", ascending=False)\n",
    "    df_audit = pd.DataFrame(audit)\n",
    "    if not df_audit.empty:\n",
    "        df_audit = df_audit.dropna(subset=[\"Fecha\"]).sort_values(\"Fecha\", ascending=False)\n",
    "\n",
    "    out_clean = \"FINAL_NYT_NEWS_5_ANIOS.CSV\"  # Resultado\n",
    "    out_audit = f\"nyt_amazon_company_{start_month_label}_to_{end_month_label}_audit.csv\"   # Bruto\n",
    "    df_clean.to_csv(out_clean, index=False, encoding=\"utf-8-sig\")\n",
    "    df_audit.to_csv(out_audit, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Resumen\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"TOTAL INCLUIDOS (clean): {len(df_clean)} -> {out_clean}\")\n",
    "    print(f\"TOTAL EXCLUIDOS (audit): {len(df_audit)} -> {out_audit}\")\n",
    "    print(\"-\"*60)\n",
    "    print(\"Conteo mensual (INCLUIDOS):\")\n",
    "    for (yy, mm) in sorted(monthly_counts.keys()):\n",
    "        print(f\"  {yy}-{mm:02d}: {monthly_counts[(yy, mm)]}\")\n",
    "    if not df_audit.empty:\n",
    "        print(\"-\"*60)\n",
    "        print(\"Top motivos de EXCLUSIÓN:\")\n",
    "        print(df_audit[\"why_excluded\"].fillna(\"\").value_counts().head(10).to_string())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752d8555",
   "metadata": {},
   "source": [
    "# 2. EXTRACCION DE NOTICIAS EODHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76e33b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] offset=    0 | filas=1000 | llamadas=1\n",
      "[OK] offset= 1000 | filas=1000 | llamadas=2\n",
      "[OK] offset= 2000 | filas=1000 | llamadas=3\n",
      "[OK] offset= 3000 | filas=1000 | llamadas=4\n",
      "[OK] offset= 4000 | filas=1000 | llamadas=5\n",
      "[OK] offset= 5000 | filas=1000 | llamadas=6\n",
      "[OK] offset= 6000 | filas=1000 | llamadas=7\n",
      "[OK] offset= 7000 | filas=1000 | llamadas=8\n",
      "[OK] offset= 8000 | filas=1000 | llamadas=9\n",
      "[OK] offset= 9000 | filas=1000 | llamadas=10\n",
      "[OK] offset=10000 | filas=1000 | llamadas=11\n",
      "[OK] offset=11000 | filas=1000 | llamadas=12\n",
      "[OK] offset=12000 | filas=1000 | llamadas=13\n",
      "[OK] offset=13000 | filas=1000 | llamadas=14\n",
      "[OK] offset=14000 | filas=1000 | llamadas=15\n",
      "[OK] offset=15000 | filas=1000 | llamadas=16\n",
      "[OK] offset=16000 | filas=1000 | llamadas=17\n",
      "[OK] offset=17000 | filas=1000 | llamadas=18\n",
      "[OK] offset=18000 | filas=1000 | llamadas=19\n",
      "[OK] offset=19000 | filas=1000 | llamadas=20\n",
      "[OK] offset=20000 | filas=1000 | llamadas=21\n",
      "[OK] offset=21000 | filas=1000 | llamadas=22\n",
      "[OK] offset=22000 | filas=1000 | llamadas=23\n",
      "[OK] offset=23000 | filas=1000 | llamadas=24\n",
      "[OK] offset=24000 | filas=1000 | llamadas=25\n",
      "[OK] offset=25000 | filas=1000 | llamadas=26\n",
      "[OK] offset=26000 | filas=1000 | llamadas=27\n",
      "[OK] offset=27000 | filas=1000 | llamadas=28\n",
      "[OK] offset=28000 | filas=1000 | llamadas=29\n",
      "[OK] offset=29000 | filas=1000 | llamadas=30\n",
      "[OK] offset=30000 | filas=1000 | llamadas=31\n",
      "[OK] offset=31000 | filas=1000 | llamadas=32\n",
      "[OK] offset=32000 | filas=1000 | llamadas=33\n",
      "[OK] offset=33000 | filas=1000 | llamadas=34\n",
      "[OK] offset=34000 | filas=1000 | llamadas=35\n",
      "[OK] offset=35000 | filas=1000 | llamadas=36\n",
      "[OK] offset=36000 | filas=1000 | llamadas=37\n",
      "[OK] offset=37000 | filas=1000 | llamadas=38\n",
      "[OK] offset=38000 | filas=1000 | llamadas=39\n",
      "[OK] offset=39000 | filas= 337 | llamadas=40\n",
      "[RAW] 39337 filas -> eodhd_news_AMZN.US_2020-07-01_2025-07-31_RAW.csv\n",
      "\n",
      "=== RECUENTO POR MES (CLEAN) ===\n",
      "2020-07: 2\n",
      "2020-08: 1\n",
      "2020-12: 2\n",
      "2021-01: 1\n",
      "2021-02: 6\n",
      "2021-03: 89\n",
      "2021-04: 205\n",
      "2021-05: 131\n",
      "2021-06: 188\n",
      "2021-07: 125\n",
      "2021-08: 99\n",
      "2021-09: 119\n",
      "2021-10: 2\n",
      "2021-11: 3\n",
      "2021-12: 1\n",
      "2022-01: 7\n",
      "2022-02: 5\n",
      "2022-03: 11\n",
      "2022-04: 12\n",
      "2022-05: 11\n",
      "2022-06: 4\n",
      "2022-07: 5\n",
      "2022-08: 6\n",
      "2022-09: 11\n",
      "2022-10: 13\n",
      "2022-11: 5\n",
      "2022-12: 1\n",
      "2023-01: 2\n",
      "2023-02: 4\n",
      "2023-03: 2\n",
      "2023-04: 1\n",
      "2023-11: 1\n",
      "2023-12: 7\n",
      "2024-01: 15\n",
      "2024-02: 8\n",
      "2024-03: 5\n",
      "2024-04: 6\n",
      "2024-05: 11\n",
      "2024-06: 12\n",
      "2024-07: 5\n",
      "2024-08: 4\n",
      "2024-09: 7\n",
      "2024-10: 4\n",
      "2024-11: 2\n",
      "2025-01: 7\n",
      "2025-02: 10\n",
      "2025-03: 2\n",
      "2025-04: 54\n",
      "2025-05: 57\n",
      "2025-06: 93\n",
      "2025-07: 111\n",
      "\n",
      "=== RESUMEN ===\n",
      "Descargadas (RAW): 39337\n",
      "Limpias (CLEAN):  1495\n",
      "Llamadas API:     40\n",
      "RAW  -> eodhd_news_AMZN.US_2020-07-01_2025-07-31_RAW.csv\n",
      "CLEAN-> FINAL_EODHD_NEWS_5_ANIOS.CSV\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "EODHD → AMZN.US  (2020-07-01 .. 2025-07-31)\n",
    "- Título debe contener 'Amazon' (palabra completa).\n",
    "- Excluye PR wires, clickbait y entretenimiento/Prime Video.\n",
    "- Excluye contexto 'Amazon rainforest' salvo señales corporativas.\n",
    "- Genera resumen extractivo y guarda RAW + CLEAN_SUM.\n",
    "\"\"\"\n",
    "\n",
    "import json, time, re, requests, pandas as pd\n",
    "from datetime import datetime, date\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# ------------------ Config ------------------\n",
    "API_TOKEN = \"DEMO\"           # DEMO funciona con AMZN.US\n",
    "TICKER    = \"AMZN.US\"\n",
    "PER_PAGE  = 1000\n",
    "SLEEP_SEC = 0.5\n",
    "BASE_URL  = \"https://eodhd.com/api/news\"\n",
    "\n",
    "DATE_FROM = date(2020, 7, 1)    # rango solicitado\n",
    "DATE_TO   = date(2025, 7, 31)   # rango solicitado\n",
    "\n",
    "# ------------------ Descarga ------------------\n",
    "def fetch_news(ticker: str, dt_from: str, dt_to: str, per_page=PER_PAGE):\n",
    "    all_rows, offset, calls = [], 0, 0\n",
    "    while True:\n",
    "        params = {\n",
    "            \"s\": ticker, \"from\": dt_from, \"to\": dt_to,\n",
    "            \"limit\": per_page, \"offset\": offset, \"fmt\": \"json\",\n",
    "            \"api_token\": API_TOKEN\n",
    "        }\n",
    "        r = requests.get(BASE_URL, params=params, timeout=30)\n",
    "        if r.status_code != 200:\n",
    "            print(f\"[ERROR] HTTP {r.status_code} -> {r.text[:300]}\")\n",
    "            r.raise_for_status()\n",
    "        txt = r.text.strip()\n",
    "        page = json.loads(txt) if txt else []\n",
    "        calls += 1\n",
    "        if not page:\n",
    "            print(f\"[OK] No hay más resultados (offset={offset}).\")\n",
    "            break\n",
    "        all_rows.extend(page)\n",
    "        print(f\"[OK] offset={offset:>5} | filas={len(page):>4} | llamadas={calls}\")\n",
    "        if len(page) < per_page:\n",
    "            break\n",
    "        offset += per_page\n",
    "        time.sleep(SLEEP_SEC)\n",
    "    return all_rows, calls\n",
    "\n",
    "rows, api_calls = fetch_news(TICKER, DATE_FROM.isoformat(), DATE_TO.isoformat())\n",
    "\n",
    "# ------------------ Flatten ------------------\n",
    "def flatten(row: dict):\n",
    "    sent = row.get(\"sentiment\") or {}\n",
    "    symbols = row.get(\"symbols\") or []\n",
    "    if isinstance(symbols, str):\n",
    "        symbols = [x.strip() for x in symbols.split(\",\") if x.strip()]\n",
    "    return {\n",
    "        \"date\": row.get(\"date\"),\n",
    "        \"title\": row.get(\"title\"),\n",
    "        \"content\": row.get(\"content\"),\n",
    "        \"link\": row.get(\"link\"),\n",
    "        \"source\": row.get(\"source\"),\n",
    "        \"lang\": row.get(\"lang\"),\n",
    "        \"symbols\": \",\".join(symbols),\n",
    "        \"tags\": \",\".join(row.get(\"tags\") or []),\n",
    "        \"polarity\": sent.get(\"polarity\"),\n",
    "        \"neg\": sent.get(\"neg\"),\n",
    "        \"neu\": sent.get(\"neu\"),\n",
    "        \"pos\": sent.get(\"pos\"),\n",
    "    }\n",
    "\n",
    "df_raw = pd.DataFrame([flatten(x) for x in rows])\n",
    "safe_from = DATE_FROM.isoformat().replace(\":\", \"-\")\n",
    "safe_to   = DATE_TO.isoformat().replace(\":\", \"-\")\n",
    "raw_path  = f\"eodhd_news_{TICKER}_{safe_from}_{safe_to}_RAW.csv\"\n",
    "df_raw.to_csv(raw_path, index=False, encoding=\"utf-8\")\n",
    "print(f\"[RAW] {len(df_raw)} filas -> {raw_path}\")\n",
    "\n",
    "# ------------------ Utilidades de limpieza ------------------\n",
    "PR_DOMAINS = [\n",
    "    \"globenewswire\", \"prnewswire\", \"businesswire\", \"accesswire\",\n",
    "    \"newsfilecorp\", \"newsdirect\", \"einnews\", \"newswire\", \"prweb\",\n",
    "    \"/press-releases\", \"/pressrelease\", \"/press-release\"\n",
    "]\n",
    "\n",
    "CLICKBAIT_PATTERNS = [\n",
    "    r\"\\bwhat to know\\b\", r\"\\bwhat we know\\b\", r\"\\beverything you need to know\\b\",\n",
    "    r\"\\byou won'?t believe\\b\", r\"\\bmust[- ]see\\b\", r\"\\bbreaking\\b\", r\"\\bgoes viral\\b\",\n",
    "    r\"\\btop\\s?\\d+\\b\", r\"\\bbest\\b\", r\"\\bhow to watch\\b\", r\"\\bwhere to watch\\b\",\n",
    "    r\"\\bstreaming guide\\b\", r\"\\brelease date\\b\", r\"\\btrailer\\b\", r\"\\brecap\\b\", r\"\\breview\\b\",\n",
    "]\n",
    "ENTERTAINMENT_PATTERNS = [\n",
    "    r\"\\bprime video\\b\", r\"\\bseries?\\b\", r\"\\bseason\\b\", r\"\\bepisode\\b\",\n",
    "    r\"\\bmr\\.?\\s*&?\\s*mrs\\.?\\s*smith\\b\", r\"\\brings?\\s*of\\s*power\\b\", r\"\\breacher\\b\",\n",
    "    r\"\\bouter range\\b\", r\"\\bthe boys\\b\", r\"\\bexpats\\b\", r\"\\bmy lady jane\\b\", r\"\\bcast\\b\"\n",
    "]\n",
    "RAINFOREST_BAD = [\n",
    "    r\"\\brainforest\\b\", r\"\\bdeforestation\\b\", r\"\\bdrought\\b\", r\"\\bamazon river\\b\",\n",
    "    r\"\\bindigenous\\b\", r\"\\btribes?\\b\"\n",
    "]\n",
    "CORP_MARKERS = [r\"\\bamazon\\.com\\b\", r\"\\baws\\b\", r\"\\bamazon web services\\b\", r\"\\bnasdaq:\\s*amzn\\b\"]\n",
    "\n",
    "def domain_from_link(link: str) -> str:\n",
    "    try:\n",
    "        return urlparse(link).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def has_pr_source(link: str, source: str) -> bool:\n",
    "    s = f\"{link} {source}\".lower()\n",
    "    return any(dom in s for dom in PR_DOMAINS)\n",
    "\n",
    "def title_has_amazon(title: str) -> bool:\n",
    "    if not isinstance(title, str): return False\n",
    "    return bool(re.search(r\"\\bamazon\\b\", title, flags=re.I))\n",
    "\n",
    "def contains_focus_terms(text: str) -> bool:\n",
    "    if not isinstance(text, str): return False\n",
    "    text = text.lower()\n",
    "    return (\"amazon\" in text) or (\"aws\" in text)\n",
    "\n",
    "def norm_title(t: str) -> str:\n",
    "    if not isinstance(t, str): return \"\"\n",
    "    t = t.lower()\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    t = re.sub(r\"[^a-z0-9 ]+\", \"\", t)\n",
    "    return t.strip()\n",
    "\n",
    "def is_clickbaity(title: str) -> bool:\n",
    "    if not isinstance(title, str): return False\n",
    "    t = title.lower()\n",
    "    short = len(t) < 28\n",
    "    bad = any(re.search(p, t) for p in CLICKBAIT_PATTERNS)\n",
    "    ent = any(re.search(p, t) for p in ENTERTAINMENT_PATTERNS)\n",
    "    return bad or ent or short\n",
    "\n",
    "def rainforest_context(text: str) -> bool:\n",
    "    if not isinstance(text, str): return False\n",
    "    t = text.lower()\n",
    "    has_rain = any(re.search(p, t) for p in RAINFOREST_BAD)\n",
    "    has_corp = any(re.search(p, t) for p in CORP_MARKERS)\n",
    "    return has_rain and not has_corp\n",
    "\n",
    "# ------------------ Resumen extractivo (sin APIs) ------------------\n",
    "_SENT_SPLIT = re.compile(r\"(?<!\\b[A-Z])(?<=[\\.\\?\\!])\\s+(?=[A-Z0-9])\")\n",
    "STOPWORDS = set(\"\"\"\n",
    "a about above after again against all am an and any are as at be because been\n",
    "before being below between both but by can did do does doing down during each few\n",
    "for from further had has have having he her here hers herself him himself his how\n",
    "i if in into is it its itself just me more most my myself no nor not of off on\n",
    "once only or other our ours ourselves out over own same she should so some such\n",
    "than that the their theirs them themselves then there these they this those through\n",
    "to too under until up very was we were what when where which while who whom why\n",
    "with you your yours yourself yourselves\n",
    "\"\"\".split())\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    if not isinstance(text, str): return []\n",
    "    t = re.sub(r\"\\s+\", \" \", text).strip().replace(\"U.S.\", \"US.\")\n",
    "    return [s.strip() for s in _SENT_SPLIT.split(t) if s.strip()]\n",
    "\n",
    "def summarize_extract(text: str, max_sents=3, max_chars=420):\n",
    "    sents = split_sentences(text)\n",
    "    if not sents: return \"\"\n",
    "    words = re.findall(r\"[a-zA-Z0-9']+\", text.lower())\n",
    "    freq = {}\n",
    "    for w in words:\n",
    "        if w in STOPWORDS or len(w) <= 2: continue\n",
    "        freq[w] = freq.get(w, 0) + 1\n",
    "    scored = []\n",
    "    for i, s in enumerate(sents):\n",
    "        tokens = re.findall(r\"[a-zA-Z0-9']+\", s.lower())\n",
    "        score = sum(freq.get(w, 0) for w in tokens)\n",
    "        scored.append((i, s, score))\n",
    "    top = sorted(scored, key=lambda x: x[2], reverse=True)[:max_sents]\n",
    "    top_sorted = sorted(top, key=lambda x: x[0])\n",
    "    out, total = [], 0\n",
    "    for _, s, _ in top_sorted:\n",
    "        add = (\"\" if not out else \" \") + s\n",
    "        if total + len(add) > max_chars: break\n",
    "        out.append(s); total += len(add)\n",
    "    if not out:\n",
    "        out = sents[:max_sents]\n",
    "    return \" \".join(out)[:max_chars].rstrip()\n",
    "\n",
    "# ------------------ Limpieza + filtros ------------------\n",
    "df = df_raw.copy()\n",
    "for col in [\"title\",\"content\",\"link\",\"source\",\"symbols\",\"tags\",\"lang\"]:\n",
    "    if col not in df.columns: df[col] = \"\"\n",
    "\n",
    "df[\"symbols_list\"]  = df[\"symbols\"].apply(lambda s: [x.strip() for x in str(s).split(\",\") if x.strip()])\n",
    "df[\"symbols_count\"] = df[\"symbols_list\"].apply(len)\n",
    "df[\"has_amzn\"]      = df[\"symbols_list\"].apply(lambda xs: \"AMZN.US\" in xs)\n",
    "df[\"title_norm\"]    = df[\"title\"].apply(norm_title)\n",
    "df[\"content_len\"]   = df[\"content\"].map(lambda x: len(str(x)))\n",
    "df[\"source_domain\"] = df[\"link\"].map(domain_from_link)\n",
    "\n",
    "mask = True\n",
    "mask = mask & df[\"title\"].apply(title_has_amazon)                                # título con 'Amazon'\n",
    "mask = mask & ~df.apply(lambda r: has_pr_source(r[\"link\"], r[\"source\"]), axis=1)  # sin PR wires\n",
    "mask = mask & df[\"has_amzn\"] & (df[\"symbols_count\"] <= 3)                         # foco en AMZN\n",
    "mask = mask & (df[\"content_len\"] >= 300)                                          # mínimo texto\n",
    "mask = mask & (df[\"title\"].apply(lambda t: not is_clickbaity(t)))                 # evita clickbait\n",
    "mask = mask & (df[\"content\"].apply(lambda t: not rainforest_context(t)))          # fuera selva no corporativa\n",
    "mask = mask & (df[\"content\"].apply(contains_focus_terms) | (df[\"content_len\"] == 0))\n",
    "\n",
    "df_f = df[mask].copy()\n",
    "\n",
    "# Deduplicación\n",
    "df_f = df_f.sort_values(\"date\").drop_duplicates(subset=[\"link\"], keep=\"first\")\n",
    "df_f = df_f.sort_values(\"date\").drop_duplicates(subset=[\"title_norm\"], keep=\"first\")\n",
    "\n",
    "# Resumen\n",
    "df_f[\"summary\"] = df_f[\"content\"].apply(lambda x: summarize_extract(str(x)))\n",
    "df_f.loc[df_f[\"summary\"].str.len() == 0, \"summary\"] = df_f[\"title\"].astype(str).str.slice(0, 240)\n",
    "\n",
    "# Guardado\n",
    "keep_cols = [\n",
    "    \"date\",\"title\",\"summary\",\"content\",\"link\",\"source_domain\",\"lang\",\n",
    "    \"symbols\",\"tags\",\"polarity\",\"neg\",\"neu\",\"pos\",\"content_len\",\"symbols_count\"\n",
    "]\n",
    "clean_path = \"FINAL_EODHD_NEWS_5_ANIOS.CSV\"  \n",
    "df_f[keep_cols].to_csv(clean_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# ------------------ Recuento por mes ------------------\n",
    "def to_month(d):\n",
    "    try:\n",
    "        return datetime.fromisoformat(str(d).replace(\"Z\",\"\")).strftime(\"%Y-%m\")\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "df_f[\"month\"] = df_f[\"date\"].apply(to_month)\n",
    "counts = df_f.groupby(\"month\").size().sort_index()\n",
    "\n",
    "print(\"\\n=== RECUENTO POR MES (CLEAN) ===\")\n",
    "if len(counts):\n",
    "    for m, c in counts.items():\n",
    "        print(f\"{m}: {c}\")\n",
    "else:\n",
    "    print(\"(vacío)\")\n",
    "\n",
    "print(\"\\n=== RESUMEN ===\")\n",
    "print(f\"Descargadas (RAW): {len(df_raw)}\")\n",
    "print(f\"Limpias (CLEAN):  {len(df_f)}\")\n",
    "print(f\"Llamadas API:     {api_calls}\")\n",
    "print(f\"RAW  -> {raw_path}\")\n",
    "print(f\"CLEAN-> {clean_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a83153",
   "metadata": {},
   "source": [
    "# 3. EXTRACCION DE NOTICIAS THE GUARDIAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b191e7ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07 [ALL] → total=13 pág(s)=1\n",
      "  ↳ acumulados 2020-07: 13\n",
      "2020-08 [ALL] → total=17 pág(s)=1\n",
      "  ↳ acumulados 2020-08: 17\n",
      "2020-09 [ALL] → total=12 pág(s)=1\n",
      "  ↳ acumulados 2020-09: 12\n",
      "2020-10 [ALL] → total=21 pág(s)=1\n",
      "  ↳ acumulados 2020-10: 21\n",
      "2020-11 [ALL] → total=19 pág(s)=1\n",
      "  ↳ acumulados 2020-11: 19\n",
      "2020-12 [ALL] → total=14 pág(s)=1\n",
      "  ↳ acumulados 2020-12: 14\n",
      "2021-01 [ALL] → total=13 pág(s)=1\n",
      "  ↳ acumulados 2021-01: 13\n",
      "2021-02 [ALL] → total=18 pág(s)=1\n",
      "  ↳ acumulados 2021-02: 18\n",
      "2021-03 [ALL] → total=17 pág(s)=1\n",
      "  ↳ acumulados 2021-03: 17\n",
      "2021-04 [ALL] → total=19 pág(s)=1\n",
      "  ↳ acumulados 2021-04: 19\n",
      "2021-05 [ALL] → total=28 pág(s)=1\n",
      "  ↳ acumulados 2021-05: 28\n",
      "2021-06 [ALL] → total=24 pág(s)=1\n",
      "  ↳ acumulados 2021-06: 24\n",
      "2021-07 [ALL] → total=14 pág(s)=1\n",
      "  ↳ acumulados 2021-07: 14\n",
      "2021-08 [ALL] → total=9 pág(s)=1\n",
      "  ↳ acumulados 2021-08: 9\n",
      "2021-09 [ALL] → total=17 pág(s)=1\n",
      "  ↳ acumulados 2021-09: 17\n",
      "2021-10 [ALL] → total=20 pág(s)=1\n",
      "  ↳ acumulados 2021-10: 20\n",
      "2021-11 [ALL] → total=21 pág(s)=1\n",
      "  ↳ acumulados 2021-11: 21\n",
      "2021-12 [ALL] → total=13 pág(s)=1\n",
      "  ↳ acumulados 2021-12: 13\n",
      "2022-01 [ALL] → total=12 pág(s)=1\n",
      "  ↳ acumulados 2022-01: 12\n",
      "2022-02 [ALL] → total=17 pág(s)=1\n",
      "  ↳ acumulados 2022-02: 17\n",
      "2022-03 [ALL] → total=14 pág(s)=1\n",
      "  ↳ acumulados 2022-03: 14\n",
      "2022-04 [ALL] → total=16 pág(s)=1\n",
      "  ↳ acumulados 2022-04: 16\n",
      "2022-05 [ALL] → total=8 pág(s)=1\n",
      "  ↳ acumulados 2022-05: 8\n",
      "2022-06 [ALL] → total=21 pág(s)=1\n",
      "  ↳ acumulados 2022-06: 21\n",
      "2022-07 [ALL] → total=18 pág(s)=1\n",
      "  ↳ acumulados 2022-07: 18\n",
      "2022-08 [ALL] → total=20 pág(s)=1\n",
      "  ↳ acumulados 2022-08: 20\n",
      "2022-09 [ALL] → total=19 pág(s)=1\n",
      "  ↳ acumulados 2022-09: 19\n",
      "2022-10 [ALL] → total=17 pág(s)=1\n",
      "  ↳ acumulados 2022-10: 17\n",
      "2022-11 [ALL] → total=17 pág(s)=1\n",
      "  ↳ acumulados 2022-11: 17\n",
      "2022-12 [ALL] → total=16 pág(s)=1\n",
      "  ↳ acumulados 2022-12: 16\n",
      "2023-01 [ALL] → total=14 pág(s)=1\n",
      "  ↳ acumulados 2023-01: 14\n",
      "2023-02 [ALL] → total=11 pág(s)=1\n",
      "  ↳ acumulados 2023-02: 11\n",
      "2023-03 [ALL] → total=11 pág(s)=1\n",
      "  ↳ acumulados 2023-03: 11\n",
      "2023-04 [ALL] → total=14 pág(s)=1\n",
      "  ↳ acumulados 2023-04: 14\n",
      "2023-05 [ALL] → total=13 pág(s)=1\n",
      "  ↳ acumulados 2023-05: 13\n",
      "2023-06 [ALL] → total=24 pág(s)=1\n",
      "  ↳ acumulados 2023-06: 24\n",
      "2023-07 [ALL] → total=9 pág(s)=1\n",
      "  ↳ acumulados 2023-07: 9\n",
      "2023-08 [ALL] → total=21 pág(s)=1\n",
      "  ↳ acumulados 2023-08: 21\n",
      "2023-09 [ALL] → total=25 pág(s)=1\n",
      "  ↳ acumulados 2023-09: 25\n",
      "2023-10 [ALL] → total=27 pág(s)=1\n",
      "  ↳ acumulados 2023-10: 27\n",
      "2023-11 [ALL] → total=13 pág(s)=1\n",
      "  ↳ acumulados 2023-11: 13\n",
      "2023-12 [ALL] → total=11 pág(s)=1\n",
      "  ↳ acumulados 2023-12: 11\n",
      "2024-01 [ALL] → total=15 pág(s)=1\n",
      "  ↳ acumulados 2024-01: 15\n",
      "2024-02 [ALL] → total=10 pág(s)=1\n",
      "  ↳ acumulados 2024-02: 10\n",
      "2024-03 [ALL] → total=7 pág(s)=1\n",
      "  ↳ acumulados 2024-03: 7\n",
      "2024-04 [ALL] → total=15 pág(s)=1\n",
      "  ↳ acumulados 2024-04: 15\n",
      "2024-05 [ALL] → total=6 pág(s)=1\n",
      "  ↳ acumulados 2024-05: 6\n",
      "2024-06 [ALL] → total=7 pág(s)=1\n",
      "  ↳ acumulados 2024-06: 7\n",
      "2024-07 [ALL] → total=16 pág(s)=1\n",
      "  ↳ acumulados 2024-07: 16\n",
      "2024-08 [ALL] → total=11 pág(s)=1\n",
      "  ↳ acumulados 2024-08: 11\n",
      "2024-09 [ALL] → total=16 pág(s)=1\n",
      "  ↳ acumulados 2024-09: 16\n",
      "2024-10 [ALL] → total=16 pág(s)=1\n",
      "  ↳ acumulados 2024-10: 16\n",
      "2024-11 [ALL] → total=17 pág(s)=1\n",
      "  ↳ acumulados 2024-11: 17\n",
      "2024-12 [ALL] → total=19 pág(s)=1\n",
      "  ↳ acumulados 2024-12: 19\n",
      "2025-01 [ALL] → total=16 pág(s)=1\n",
      "  ↳ acumulados 2025-01: 16\n",
      "2025-02 [ALL] → total=14 pág(s)=1\n",
      "  ↳ acumulados 2025-02: 14\n",
      "2025-03 [ALL] → total=21 pág(s)=1\n",
      "  ↳ acumulados 2025-03: 21\n",
      "2025-04 [ALL] → total=8 pág(s)=1\n",
      "  ↳ acumulados 2025-04: 8\n",
      "2025-05 [ALL] → total=21 pág(s)=1\n",
      "  ↳ acumulados 2025-05: 21\n",
      "2025-06 [ALL] → total=14 pág(s)=1\n",
      "  ↳ acumulados 2025-06: 14\n",
      "2025-07 [ALL] → total=14 pág(s)=1\n",
      "  ↳ acumulados 2025-07: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_40324\\1327233339.py:150: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df = df[df[\"Titular\"].str.contains(RE_BROAD, na=False)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESUMEN ===\n",
      "Total únicos (filtrados): 349\n",
      "Guardado: FINAL_THE_GUARDIAN_NEWS_5_ANIOS.CSV\n"
     ]
    }
   ],
   "source": [
    "# guardian_fetch_amazon.py\n",
    "import requests, pandas as pd, re, time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from calendar import monthrange\n",
    "from datetime import date\n",
    "\n",
    "# Clave eliminada por seguridad para publicación en repositorio\n",
    "GUARDIAN_API_KEY = \"TU_API_KEY_AQUI\" \n",
    "\n",
    "# Secciones: vacío para máximo recall (filtra luego por contenido)\n",
    "SECCIONES = []\n",
    "\n",
    "# Términos buscados solo en titulares\n",
    "BROAD_TERMS = [\n",
    "    \"amazon\",\"amzn\",\"aws\",\"prime video\",\"alexa\",\"ring\",\"kindle\",\"whole foods\",\n",
    "    \"zoox\",\"irobot\",\"mgm\",\"kuiper\",\"project kuiper\",\"prime day\",\"amazon fresh\",\n",
    "    \"amazon go\",\"twitch\",\"audible\"\n",
    "]\n",
    "\n",
    "# Filtros de titular/contenido\n",
    "RE_STRICT = re.compile(r\"\\bamazon\\b\", re.IGNORECASE)\n",
    "RE_BROAD  = re.compile(\n",
    "    r\"\\b(amazon|amzn|aws|prime video|alexa|ring|kindle|whole foods|zoox|irobot|mgm|kuiper|project kuiper|prime day|amazon fresh|amazon go|twitch|audible)\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Anti-clickbait/entretenimiento\n",
    "RE_CLICKBAIT = re.compile(\n",
    "    r\"\\b(what to know|what we know|everything you need to know|won'?t believe|must[- ]see|breaking|goes viral|\"\n",
    "    r\"top\\s?\\d+|best|how to watch|where to watch|streaming guide|release date|trailer|recap|review|minute[- ]by[- ]minute)\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "RE_ENT = re.compile(\n",
    "    r\"\\b(prime video|series?|season|episode|cast|the boys|rings? of power|reacher|outer range|expats|mr\\.?\\s*&?\\s*mrs\\.?\\s*smith)\\b\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "# Selva Amazónica (ruido) y marcadores corporativos (permiten)\n",
    "RE_RAINFOREST = re.compile(r\"\\b(rainforest|deforestation|drought|amazon river|indigenous|tribes?)\\b\", re.IGNORECASE)\n",
    "RE_CORP = re.compile(r\"\\b(amazon\\.com|aws|amazon web services|nasdaq:\\s*amzn|earnings|revenue|guidance|nlrb|osha|warehouse|warehouses|fulfillment|prime day|marketplace|advertising|cloud|kuiper|data ?center|ai)\\b\", re.IGNORECASE)\n",
    "\n",
    "# Blue Origin solo con Kuiper/satélite Amazon\n",
    "RE_BLUE_ORIGIN = re.compile(r\"\\bblue origin\\b\", re.IGNORECASE)\n",
    "RE_KUIPER = re.compile(r\"\\b(kuiper|amazon(?:’|')?s satellite|amazon satellite|project kuiper)\\b\", re.IGNORECASE)\n",
    "\n",
    "BASE_URL   = \"https://content.guardianapis.com/search\"\n",
    "START_DATE = date(2020, 7, 1)   # 01-07-2020\n",
    "END_DATE   = date(2025, 7, 31)  # 31-07-2025\n",
    "\n",
    "def _query_string(terms):\n",
    "    # Construye query OR con comillas cuando hay espacios\n",
    "    parts = [f'\"{t.strip()}\"' if \" \" in t else t.strip() for t in terms]\n",
    "    return \" OR \".join(parts)\n",
    "\n",
    "def _bad_entertainment(title, text):\n",
    "    # Marca entretenimiento/clickbait\n",
    "    return bool(RE_CLICKBAIT.search(title) or RE_ENT.search(title) or RE_ENT.search(text))\n",
    "\n",
    "def _rainforest_without_corp(title, text):\n",
    "    # Selva sin marcadores corporativos\n",
    "    t = f\"{title}\\n{text}\"\n",
    "    return bool(RE_RAINFOREST.search(t) and not RE_CORP.search(t))\n",
    "\n",
    "def _blue_origin_without_kuiper(title, text):\n",
    "    # Blue Origin sin Kuiper/satélite Amazon\n",
    "    t = f\"{title}\\n{text}\"\n",
    "    return bool(RE_BLUE_ORIGIN.search(t) and not RE_KUIPER.search(t))\n",
    "\n",
    "def fetch_guardian():\n",
    "    if not GUARDIAN_API_KEY or GUARDIAN_API_KEY == \"TU_API_KEY_AQUI\":\n",
    "        raise SystemExit(\"❌ Añade tu API key en GUARDIAN_API_KEY.\")\n",
    "\n",
    "    q = _query_string(BROAD_TERMS)\n",
    "\n",
    "    rows = []\n",
    "    cursor = pd.Timestamp(START_DATE.year, START_DATE.month, 1)\n",
    "    endm   = pd.Timestamp(END_DATE.year, END_DATE.month, 1)\n",
    "\n",
    "    # Recorre meses del rango\n",
    "    while cursor <= endm:\n",
    "        y, m = cursor.year, cursor.month\n",
    "        from_date = f\"{y}-{m:02d}-01\"\n",
    "        to_date   = f\"{y}-{m:02d}-{monthrange(y, m)[1]:02d}\"\n",
    "\n",
    "        secciones = SECCIONES if SECCIONES else [None]\n",
    "        total_mes = 0\n",
    "\n",
    "        for sec in secciones:\n",
    "            page, pages = 1, 1\n",
    "            while page <= pages:\n",
    "                params = {\n",
    "                    \"api-key\": GUARDIAN_API_KEY,\n",
    "                    \"q\": q,                              # términos\n",
    "                    \"query-fields\": \"headline\",          # solo titulares\n",
    "                    \"type\": \"article\",                   # fuera liveblogs\n",
    "                    \"from-date\": from_date,\n",
    "                    \"to-date\": to_date,\n",
    "                    \"page-size\": 200,\n",
    "                    \"page\": page,\n",
    "                    \"order-by\": \"newest\",\n",
    "                    \"use-date\": \"published\",\n",
    "                    \"show-fields\": \"headline,trailText,standfirst,bodyText\"\n",
    "                }\n",
    "                if sec:\n",
    "                    params[\"section\"] = sec\n",
    "\n",
    "                try:\n",
    "                    r = requests.get(BASE_URL, params=params, timeout=30)\n",
    "                    r.raise_for_status()\n",
    "                except requests.RequestException as e:\n",
    "                    print(f\"  ! Error {cursor.strftime('%Y-%m')} pág {page}: {e}\")\n",
    "                    break\n",
    "\n",
    "                resp  = r.json().get(\"response\", {})\n",
    "                pages = resp.get(\"pages\", 1)\n",
    "                res   = resp.get(\"results\", [])\n",
    "\n",
    "                if page == 1:\n",
    "                    sec_txt = sec if sec else \"ALL\"\n",
    "                    print(f\"{cursor.strftime('%Y-%m')} [{sec_txt}] → total={resp.get('total',0)} pág(s)={pages}\")\n",
    "\n",
    "                for it in res:\n",
    "                    title = it.get(\"webTitle\",\"\") or \"\"\n",
    "                    text  = it.get(\"fields\",{}).get(\"bodyText\",\"\") or \"\"\n",
    "                    rows.append({\n",
    "                        \"Fecha\":   it.get(\"webPublicationDate\"),\n",
    "                        \"Seccion\": it.get(\"sectionName\"),\n",
    "                        \"Titular\": title,\n",
    "                        \"URL\":     it.get(\"webUrl\",\"\") or \"\",\n",
    "                        \"Texto\":   text\n",
    "                    })\n",
    "                    total_mes += 1\n",
    "\n",
    "                page += 1\n",
    "                time.sleep(0.12)  # cortesía\n",
    "\n",
    "        print(f\"  ↳ acumulados {cursor.strftime('%Y-%m')}: {total_mes}\")\n",
    "        cursor += relativedelta(months=1)\n",
    "\n",
    "    if not rows:\n",
    "        raise SystemExit(\"Sin resultados.\")\n",
    "\n",
    "    # Limpieza y rango fijo\n",
    "    df = pd.DataFrame(rows).drop_duplicates(subset=[\"URL\"])\n",
    "    df[\"Fecha\"] = pd.to_datetime(df[\"Fecha\"], utc=True).dt.tz_convert(None)\n",
    "    df = df[(df[\"Fecha\"] >= pd.Timestamp(START_DATE)) & (df[\"Fecha\"] <= pd.Timestamp(END_DATE))]\n",
    "\n",
    "    # Filtro principal: titular debe contener 'amazon' exacto\n",
    "    df = df[df[\"Titular\"].str.contains(RE_STRICT, na=False)]\n",
    "    # También exige presencia de términos ampliados en titular\n",
    "    df = df[df[\"Titular\"].str.contains(RE_BROAD, na=False)]\n",
    "\n",
    "    # Filtros negativos (menos ruido)\n",
    "    df = df[~df.apply(lambda r: _bad_entertainment(r[\"Titular\"], r[\"Texto\"]), axis=1)]\n",
    "    df = df[~df.apply(lambda r: _rainforest_without_corp(r[\"Titular\"], r[\"Texto\"]), axis=1)]\n",
    "    df = df[~df.apply(lambda r: _blue_origin_without_kuiper(r[\"Titular\"], r[\"Texto\"]), axis=1)]\n",
    "\n",
    "    # Orden final\n",
    "    df = df.sort_values(\"Fecha\", ascending=False)\n",
    "\n",
    "    # Guardado único con el nombre solicitado\n",
    "    out_path = \"FINAL_THE_GUARDIAN_NEWS_5_ANIOS.CSV\"\n",
    "    df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    # Resumen\n",
    "    print(\"\\n=== RESUMEN ===\")\n",
    "    print(f\"Total únicos (filtrados): {len(df):,}\")\n",
    "    print(f\"Guardado: {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_guardian()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe9870",
   "metadata": {},
   "source": [
    "# 4. Unificar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83621aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listo: C:\\Users\\gabri\\OneDrive\\Desktop\\Desktop\\UNIR\\TFM\\Codigo\\NOTICIAS_UNIFICADAS_5_ANIOS.csv  (2,469 filas)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Une los tres CSV de noticias (EODHD, NYT, The Guardian) en un único fichero normalizado.\n",
    "# Requisitos: pandas (pip install pandas)\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuración mínima ---\n",
    "RUTA_BASE = Path(\".\")  \n",
    "ARCHIVOS = [\n",
    "    RUTA_BASE / \"FINAL_EODHD_NEWS_5_ANIOS.CSV\",\n",
    "    RUTA_BASE / \"FINAL_NYT_NEWS_5_ANIOS.CSV\",\n",
    "    RUTA_BASE / \"FINAL_THE_GUARDIAN_NEWS_5_ANIOS.CSV\",\n",
    "]\n",
    "SALIDA = RUTA_BASE / \"NOTICIAS_UNIFICADAS_5_ANIOS.csv\"\n",
    "\n",
    "CANDIDATOS_FECHA = [\n",
    "    \"Fecha\",\"fecha\",\"date\",\"published\",\"pub_date\",\"publication_date\",\n",
    "    \"webPublicationDate\",\"time\",\"created_at\",\"created_utc\",\"datetime\"\n",
    "]\n",
    "CANDIDATOS_TITULO = [\n",
    "    \"Titular\",\"titular\",\"title\",\"headline.main\",\"headline_main\",\"headline\",\n",
    "    \"webTitle\",\"heading\",\"name\"\n",
    "]\n",
    "CANDIDATOS_RESUMEN = [\n",
    "    \"resumen\",\"summary\",\"abstract\",\"snippet\",\"lead_paragraph\",\"trailText\",\n",
    "    \"standfirst\",\"description\",\"dek\"\n",
    "]\n",
    "\n",
    "def _leer_csv(carpeta_o_fichero: Path) -> pd.DataFrame:\n",
    "    \"\"\"Se intenta con varias codificaciones; se delega el resto a pandas.\"\"\"\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin-1\"):\n",
    "        try:\n",
    "            return pd.read_csv(carpeta_o_fichero, encoding=enc)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "  \n",
    "    return pd.read_csv(carpeta_o_fichero, engine=\"python\")\n",
    "\n",
    "def _col(df: pd.DataFrame, candidatos: list[str]) -> str | None:\n",
    "    \"\"\"Se localiza la primera columna existente según una lista de candidatos (case-insensitive).\"\"\"\n",
    "    mapa = {c.lower(): c for c in df.columns}\n",
    "    # Búsqueda exacta (insensible a mayúsculas)\n",
    "    for c in candidatos:\n",
    "        if c.lower() in mapa:\n",
    "            return mapa[c.lower()]\n",
    "    # Búsqueda por inclusión (por si viene con prefijos/sufijos inesperados)\n",
    "    for c in candidatos:\n",
    "        for k in mapa:\n",
    "            if c.lower() in k:\n",
    "                return mapa[k]\n",
    "    return None\n",
    "\n",
    "_TAGS = re.compile(r\"<[^>]+>\")\n",
    "def _limpiar_html(x: str) -> str:\n",
    "    \"\"\"Se eliminan etiquetas HTML básicas en resúmenes/entradillas.\"\"\"\n",
    "    if not isinstance(x, str):\n",
    "        return \"\"\n",
    "    return _TAGS.sub(\"\", x).strip()\n",
    "\n",
    "def _normalizar(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Se normaliza a columnas: Fecha (ISO), Titular, resumen.\"\"\"\n",
    "    c_fecha = _col(df, CANDIDATOS_FECHA)\n",
    "    c_tit   = _col(df, CANDIDATOS_TITULO)\n",
    "    c_res   = _col(df, CANDIDATOS_RESUMEN)\n",
    "\n",
    "    # Se crean columnas seguras aunque falten en origen\n",
    "    tmp = pd.DataFrame()\n",
    "    if c_fecha is None:\n",
    "        # Sin fecha no se puede integrar: se descarta en bloque\n",
    "        return tmp\n",
    "\n",
    "    tmp[\"Fecha\"] = pd.to_datetime(df[c_fecha], errors=\"coerce\", utc=True)\n",
    "\n",
    "    if c_tit is not None:\n",
    "        tmp[\"Titular\"] = df[c_tit].astype(str).str.strip()\n",
    "    else:\n",
    "        tmp[\"Titular\"] = \"\"\n",
    "\n",
    "    if c_res is not None:\n",
    "        tmp[\"resumen\"] = df[c_res].astype(str).map(_limpiar_html)\n",
    "    else:\n",
    "        tmp[\"resumen\"] = \"\"\n",
    "\n",
    "    # Se filtra lo no convertible a fecha y filas totalmente vacías de texto\n",
    "    tmp = tmp.dropna(subset=[\"Fecha\"])\n",
    "    vacias = (tmp[\"Titular\"].str.len() == 0) & (tmp[\"resumen\"].str.len() == 0)\n",
    "    tmp = tmp[~vacias]\n",
    "\n",
    "    return tmp\n",
    "\n",
    "def main():\n",
    "    # Se cargan, normalizan y apilan las tres fuentes heterogéneas\n",
    "    marcos = []\n",
    "    for f in ARCHIVOS:\n",
    "        df_raw = _leer_csv(f)\n",
    "        marcos.append(_normalizar(df_raw))\n",
    "\n",
    "    df = pd.concat(marcos, ignore_index=True)\n",
    "\n",
    "    # Se ordena por fecha y se eliminan duplicados evidentes\n",
    "    df = df.sort_values(\"Fecha\").drop_duplicates(subset=[\"Fecha\", \"Titular\", \"resumen\"])\n",
    "\n",
    "    # Se exporta con fecha en ISO 8601 (manteniendo hora si existe)\n",
    "    df[\"Fecha\"] = df[\"Fecha\"].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    df = df[[\"Fecha\", \"Titular\", \"resumen\"]]\n",
    "\n",
    "    df.to_csv(SALIDA, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Listo: {SALIDA.resolve()}  ({len(df):,} filas)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcd1132",
   "metadata": {},
   "source": [
    "LIMPIEZA MANUAL: Amazonas, clickbait, eliminadas las que no tienen resumen\n",
    "\n",
    "Van 2051\n",
    "\n",
    "NOTICIAS_UNIFICADAS_5_ANIOS - V2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec66199",
   "metadata": {},
   "source": [
    "Codigo siguiente marca relevantes: \n",
    "\n",
    "“Si” (relevante para AMZN): resultados trimestrales, guidance, márgenes/KPIs (p. ej., AWS), M&A/ventas de activos, cambios de precios (Prime/AWS), grandes contratos/alianzas, outages o incidentes de seguridad en AWS, decisiones regulatorias (FTC/UE), cambios directivos o despidos masivos, huelgas/logística con impacto amplio.\n",
    "\n",
    "“No” (no mueve la cotización): noticias del Amazonas/Amazonía (selva/región), eventos locales/menores (una nave, una ciudad, una acción benéfica), clickbait/tutoriales/cupones/reseñas, notas genéricas del sector sin vínculo material con Amazon, resúmenes duplicados o sin novedad, títulos vagos o sin resumen.\n",
    "\n",
    "Ambigüas → criterio conservador: si no queda claro el impacto financiero o no se menciona Amazon/AWS/Prime/negocios asociados de forma explícita, se marca “No”.\n",
    "\n",
    "Idea-fuerza: solo se etiqueta “Si” cuando el contenido podría mover el precio o refleja cambios materiales en el negocio de Amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccd2ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500/2048] guardado parcial… (llamadas API: 462)\n",
      "[1000/2048] guardado parcial… (llamadas API: 902)\n",
      "[1500/2048] guardado parcial… (llamadas API: 1337)\n",
      "[2000/2048] guardado parcial… (llamadas API: 1809)\n",
      "Guardado: C:\\Users\\gabri\\OneDrive\\Desktop\\Desktop\\UNIR\\TFM\\Codigo\\NOTICIAS_UNIFICADAS_5_ANIOS - V2 - con Relevante.csv\n",
      "Filas: 2048 | Si=1201 No=847 | Llamadas a API: 1854\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Clasifica relevancia para AMZN en TODO el dataset y añade 'Relevante' (Si/No) a cada fila.\n",
    "# Requisitos: pip install openai pandas\n",
    "\n",
    "import os, re, json, time, hashlib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- Configuración ---\n",
    "INPUT_FILE  = Path(\"NOTICIAS_UNIFICADAS_5_ANIOS - V2.csv\")\n",
    "OUTPUT_FILE = INPUT_FILE.with_name(f\"{INPUT_FILE.stem} - con Relevante.csv\")\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "MAX_OUTPUT_TOKENS = 8\n",
    "SAVE_EVERY = 500  # guarda progreso cada X filas\n",
    "# Clave eliminada por seguridad para publicación en repositorio\n",
    "API_KEY = \"TU_API_KEY_AQUI\"\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "# --- Reglas fijas (prefijo estable para aprovechar cache del servidor) ---\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a finance/news relevance classifier. Decide if a news item is relevant to Amazon.com, Inc.’s stock (AMZN).\n",
    "Output EXACTLY one JSON: {\"Relevante\":\"Si\"} or {\"Relevante\":\"No\"} — in Spanish, initial capital S/N, no extra fields.\n",
    "\n",
    "Label “Si” if it plausibly moves AMZN or reflects material fundamentals/risks:\n",
    "- Earnings, revenue, guidance, KPIs (AWS growth/margins), buybacks/dividends.\n",
    "- Major AWS events (outages, security, pricing, large client wins/losses, AI/infra with clear scale).\n",
    "- M&A/divestitures/investments (e.g., Project Kuiper milestones with commercial impact).\n",
    "- Government/regulatory/legal actions materially involving Amazon (FTC/DoJ/EU, big fines, union rulings, targeted taxation).\n",
    "- Leadership/board changes, large layoffs/hiring waves, strategy shifts.\n",
    "- Prime/retail logistics with clear financial impact (Prime price change, nationwide strikes, broad delivery disruptions).\n",
    "- Large partnerships/contracts with quantified or clearly material scope.\n",
    "\n",
    "Label “No” if:\n",
    "- It is about the Amazon rainforest/region (“Amazonas”, “Amazonía”, “Manaus”, deforestation, indigenous communities…).\n",
    "- Local/minor items (single warehouse accidents, local charity/store opening) without broad financial impact.\n",
    "- Clickbait, listicles, coupons, how-to guides, product reviews, marketing fluff, celebrity gossip.\n",
    "- Generic industry news without specific material link to Amazon.\n",
    "- Duplicate/trivial updates with no new material info.\n",
    "- Missing summary and vague title with no material signal.\n",
    "\n",
    "Be conservative: default to “No” unless materiality is clear.\n",
    "\n",
    "INPUT:\n",
    "Title: <string>\n",
    "Summary: <string or empty>\n",
    "\n",
    "OUTPUT (strict):\n",
    "{\"Relevante\":\"Si\"}  OR  {\"Relevante\":\"No\"}\n",
    "\"\"\".strip()\n",
    "\n",
    "# --- Prefiltro local (reduce llamadas obvias) ---\n",
    "RE_RAINFOREST = re.compile(r\"\\b(amazonas|amazonía|amazônia|manaus|rainforest|amazon basin|amaz[oó]n.+forest)\\b\", re.I)\n",
    "RE_CLICKBAIT  = re.compile(r\"\\b(how to|guide|coupon|deal|promo|trick|tips|hacks|ranking|top \\d+|you won.?t believe)\\b\", re.I)\n",
    "RE_LOCAL      = re.compile(r\"\\b(local|community|neighborhood|parish|county fair|charity 5k|school fundraiser)\\b\", re.I)\n",
    "RE_COMPANY    = re.compile(\n",
    "    r\"\\b(amazon\\.com|amzn|amazon|aws|prime( video)?|whole foods|ring|twitch|kindle|zoox|irobot|mgm|audible|kuiper|project kuiper|amazon go|amazon fresh)\\b\",\n",
    "    re.I,\n",
    ")\n",
    "\n",
    "def cheap_prefilter(title: str, summary: str) -> str | None:\n",
    "    \"\"\"Marca 'No' si es irrelevante evidente; None si es dudoso.\"\"\"\n",
    "    t = (title or \"\").strip()\n",
    "    s = (summary or \"\").strip()\n",
    "    txt = f\"{t} {s}\"\n",
    "    if RE_RAINFOREST.search(txt): return \"No\"\n",
    "    if RE_CLICKBAIT.search(txt):  return \"No\"\n",
    "    if RE_LOCAL.search(txt):      return \"No\"\n",
    "    if not RE_COMPANY.search(txt):# si no menciona la compañía/entorno directo\n",
    "        return \"No\"\n",
    "    if not t and not s:\n",
    "        return \"No\"\n",
    "    return None\n",
    "\n",
    "# --- Cache local por duplicados (evita pagar dos veces por el mismo texto) ---\n",
    "def key_for(title: str, summary: str) -> str:\n",
    "    h = hashlib.sha256(f\"{title}||{summary}\".encode(\"utf-8\")).hexdigest()\n",
    "    return h\n",
    "\n",
    "def classify_with_openai(title: str, summary: str, retry=3) -> str:\n",
    "    \"\"\"Llama a OpenAI con salida JSON estricta y devuelve 'Si'/'No'.\"\"\"\n",
    "    for attempt in range(retry):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                temperature=0,\n",
    "                max_tokens=MAX_OUTPUT_TOKENS,\n",
    "                response_format={\"type\": \"json_object\"},\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": f\"Title: {title}\\nSummary: {summary or ''}\"}\n",
    "                ],\n",
    "            )\n",
    "            content = (resp.choices[0].message.content or \"\").strip()\n",
    "            data = json.loads(content) if content.startswith(\"{\") else {}\n",
    "            val = data.get(\"Relevante\", \"No\")\n",
    "            return \"Si\" if val == \"Si\" else \"No\"\n",
    "        except Exception:\n",
    "            time.sleep(1.5 * (attempt + 1))\n",
    "    return \"No\"\n",
    "\n",
    "def read_csv_any(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Lee el CSV completo preservando columnas.\"\"\"\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin-1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return pd.read_csv(path, engine=\"python\")\n",
    "\n",
    "def main():\n",
    "    df = read_csv_any(INPUT_FILE).copy()\n",
    "    col_fecha   = \"Fecha\"   if \"Fecha\"   in df.columns else df.columns[0]\n",
    "    col_titular = \"Titular\" if \"Titular\" in df.columns else df.columns[1]\n",
    "    col_resumen = \"resumen\" if \"resumen\" in df.columns else df.columns[2]\n",
    "\n",
    "    if \"Relevante\" not in df.columns:\n",
    "        df[\"Relevante\"] = \"\"\n",
    "\n",
    "    cache: dict[str, str] = {}\n",
    "    api_calls = 0\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        if df.at[i, \"Relevante\"]:\n",
    "            continue  # respeta valores ya existentes si relanzas\n",
    "\n",
    "        title = str(row.get(col_titular, \"\") or \"\")\n",
    "        summ  = str(row.get(col_resumen, \"\") or \"\")\n",
    "\n",
    "        # Prefiltro\n",
    "        pre = cheap_prefilter(title, summ)\n",
    "        if pre is not None:\n",
    "            df.at[i, \"Relevante\"] = pre\n",
    "        else:\n",
    "            # Cache por contenido\n",
    "            k = key_for(title, summ)\n",
    "            if k in cache:\n",
    "                df.at[i, \"Relevante\"] = cache[k]\n",
    "            else:\n",
    "                api_calls += 1\n",
    "                label = classify_with_openai(title, summ)\n",
    "                cache[k] = label\n",
    "                df.at[i, \"Relevante\"] = label\n",
    "\n",
    "        # Guardado incremental\n",
    "        if (i + 1) % SAVE_EVERY == 0:\n",
    "            df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "            print(f\"[{i+1}/{len(df)}] guardado parcial… (llamadas API: {api_calls})\")\n",
    "\n",
    "    # Guardado final\n",
    "    df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "    si = int((df[\"Relevante\"] == \"Si\").sum())\n",
    "    no = int((df[\"Relevante\"] == \"No\").sum())\n",
    "    print(f\"Guardado: {OUTPUT_FILE.resolve()}\")\n",
    "    print(f\"Filas: {len(df)} | Si={si} No={no} | Llamadas a API: {api_calls}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e3b3ae",
   "metadata": {},
   "source": [
    "Introduzco sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a86f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progreso real: 25/1201 | tokens prompt=8953 completion=46\n",
      "Progreso real: 50/1201 | tokens prompt=17814 completion=91\n",
      "Progreso real: 75/1201 | tokens prompt=27345 completion=128\n",
      "Progreso real: 100/1201 | tokens prompt=37102 completion=172\n",
      "Progreso real: 125/1201 | tokens prompt=46588 completion=218\n",
      "Progreso real: 150/1201 | tokens prompt=56376 completion=263\n",
      "Progreso real: 175/1201 | tokens prompt=65889 completion=303\n",
      "Progreso real: 200/1201 | tokens prompt=75642 completion=340\n",
      "Progreso real: 225/1201 | tokens prompt=85446 completion=376\n",
      "Progreso real: 250/1201 | tokens prompt=95122 completion=414\n",
      "Progreso real: 275/1201 | tokens prompt=104887 completion=440\n",
      "Progreso real: 300/1201 | tokens prompt=114694 completion=481\n",
      "Progreso real: 325/1201 | tokens prompt=124528 completion=516\n",
      "Progreso real: 350/1201 | tokens prompt=134059 completion=555\n",
      "Progreso real: 375/1201 | tokens prompt=143915 completion=588\n",
      "Progreso real: 400/1201 | tokens prompt=153533 completion=629\n",
      "Progreso real: 425/1201 | tokens prompt=163224 completion=667\n",
      "Progreso real: 450/1201 | tokens prompt=172949 completion=706\n",
      "Progreso real: 475/1201 | tokens prompt=182754 completion=741\n",
      "Progreso real: 500/1201 | tokens prompt=192232 completion=777\n",
      "Progreso real: 525/1201 | tokens prompt=202010 completion=815\n",
      "Progreso real: 550/1201 | tokens prompt=211440 completion=855\n",
      "Progreso real: 575/1201 | tokens prompt=221149 completion=901\n",
      "Progreso real: 600/1201 | tokens prompt=230848 completion=944\n",
      "Progreso real: 625/1201 | tokens prompt=240656 completion=985\n",
      "Progreso real: 650/1201 | tokens prompt=250574 completion=1019\n",
      "Progreso real: 675/1201 | tokens prompt=260386 completion=1057\n",
      "Progreso real: 700/1201 | tokens prompt=270190 completion=1093\n",
      "Progreso real: 725/1201 | tokens prompt=279610 completion=1134\n",
      "Progreso real: 750/1201 | tokens prompt=288743 completion=1179\n",
      "Progreso real: 775/1201 | tokens prompt=297736 completion=1222\n",
      "Progreso real: 800/1201 | tokens prompt=307124 completion=1268\n",
      "Progreso real: 825/1201 | tokens prompt=316392 completion=1310\n",
      "Progreso real: 850/1201 | tokens prompt=326005 completion=1357\n",
      "Progreso real: 875/1201 | tokens prompt=335136 completion=1397\n",
      "Progreso real: 900/1201 | tokens prompt=344082 completion=1437\n",
      "Progreso real: 925/1201 | tokens prompt=353674 completion=1468\n",
      "Progreso real: 950/1201 | tokens prompt=362941 completion=1505\n",
      "Progreso real: 975/1201 | tokens prompt=372150 completion=1543\n",
      "Progreso real: 1000/1201 | tokens prompt=381638 completion=1585\n",
      "Progreso real: 1025/1201 | tokens prompt=391361 completion=1628\n",
      "Progreso real: 1050/1201 | tokens prompt=401177 completion=1659\n",
      "Progreso real: 1075/1201 | tokens prompt=411042 completion=1690\n",
      "Progreso real: 1100/1201 | tokens prompt=420832 completion=1723\n",
      "Progreso real: 1125/1201 | tokens prompt=430707 completion=1759\n",
      "Progreso real: 1150/1201 | tokens prompt=440535 completion=1793\n",
      "Progreso real: 1175/1201 | tokens prompt=450284 completion=1830\n",
      "Progreso real: 1200/1201 | tokens prompt=460105 completion=1862\n",
      "Guardado: C:\\Users\\gabri\\OneDrive\\Desktop\\Desktop\\UNIR\\TFM\\Codigo\\NOTICIAS_UNIFICADAS_5_ANIOS - V3 - puntuado_200.csv\n",
      "Tokens usados: prompt=460521, completion=1864, total=462385\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Puntúa –10..+10 el impacto para AMZN en las primeras N_ROWS noticias.\n",
    "# Requisitos: pip install openai pandas\n",
    "\n",
    "import re, json, time, sys, traceback\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "# =============== CONFIG =================\n",
    "INPUT_FILE  = Path(\"NOTICIAS_UNIFICADAS_5_ANIOS - V3.csv\")\n",
    "OUTPUT_FILE = INPUT_FILE.with_name(f\"{INPUT_FILE.stem} - puntuado_200.csv\")\n",
    "N_ROWS = None           # pon None para TODO el dataset\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "MAX_TOKENS = 8         # solo necesitamos un entero corto\n",
    "\n",
    "# Clave eliminada por seguridad para publicación en repositorio\n",
    "API_KEY = \"TU_API_KEY_AQUI\"\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "# ===== Prompt (sin JSON): devuelve SOLO un entero -10..10 =====\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a financial impact rater for Amazon.com, Inc. (AMZN).\n",
    "Return ONLY one integer from -10 to +10 (no words). Positive = good for AMZN; negative = bad.\n",
    "Bigger |score| = more material (magnitude, breadth/duration, directness to AMZN/AWS/Prime/Whole Foods/Kuiper, novelty, credibility).\n",
    "\n",
    "Scale (guidance):\n",
    "+9..+10 blockbuster beat/guidance raise; multi-billion contract; antitrust case dismissed; big buyback/dividend.\n",
    "+6..+8  clear beat; AWS acceleration; major partnership/pricing with numbers; accretive M&A.\n",
    "+3..+5  moderate positive; some numbers/scope.\n",
    "+1..+2  mild/uncertain positive.\n",
    " 0       neutral/ambiguous; rainforest/region “Amazonas/Amazonía/Manaus”.\n",
    "-1..-2  mild/uncertain negative.\n",
    "-3..-5  miss/guidance trim; sizable fine; regional strike; slowdown datapoint.\n",
    "-6..-8  major negative: AWS outage/security; antitrust/regulatory action; broad labor disruption; big breach; guidance cut.\n",
    "-9..-10 severe negative: multi-region/multi-day outage; lost landmark case with heavy remedies; breakup-like remedy; huge recurring costs/taxation.\n",
    "\"\"\".strip()\n",
    "# =======================================\n",
    "\n",
    "def read_csv_any(path: Path, nrows=None) -> pd.DataFrame:\n",
    "    \"\"\"Lee con fallback de codificación.\"\"\"\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin-1\"):\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, nrows=nrows)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return pd.read_csv(path, engine=\"python\", nrows=nrows)\n",
    "\n",
    "def usage_to_dict(usage) -> dict:\n",
    "    if not usage:\n",
    "        return {}\n",
    "    d = {}\n",
    "    for k in (\"prompt_tokens\", \"completion_tokens\", \"total_tokens\"):\n",
    "        v = getattr(usage, k, None)\n",
    "        if v is not None:\n",
    "            d[k] = v\n",
    "    return d\n",
    "\n",
    "def score_with_openai(title: str, summary: str, retry: int = 3):\n",
    "    \"\"\"\n",
    "    Devuelve (puntuacion:int, usage:dict). En fallo, (0, {}).\n",
    "    Sin JSON; se extrae el primer entero de la respuesta.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "    for attempt in range(retry):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                temperature=0,\n",
    "                max_tokens=MAX_TOKENS,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": f\"Title: {title}\\nSummary: {summary or ''}\\nAnswer with a single integer between -10 and 10. Return only the number.\"}\n",
    "                ],\n",
    "            )\n",
    "            content = (resp.choices[0].message.content or \"\").strip()\n",
    "            usage = usage_to_dict(getattr(resp, \"usage\", None))\n",
    "\n",
    "            m = re.search(r\"-?\\d+\", content)  # primer entero\n",
    "            val = int(m.group()) if m else 0\n",
    "            val = max(-10, min(10, val))      # saturación a rango\n",
    "            return val, usage\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(1.2 * (attempt + 1))\n",
    "    print(\"[WARN] 0 por fallo tras reintentos. Último error:\")\n",
    "    traceback.print_exception(type(last_err), last_err, None)\n",
    "    return 0, {}\n",
    "\n",
    "def main():\n",
    "    # Carga parcial o total\n",
    "    df = read_csv_any(INPUT_FILE, nrows=N_ROWS).copy() if N_ROWS else read_csv_any(INPUT_FILE).copy()\n",
    "\n",
    "    # Columnas esperadas\n",
    "    col_titular = \"Titular\" if \"Titular\" in df.columns else df.columns[1]\n",
    "    col_resumen = \"resumen\" if \"resumen\" in df.columns else df.columns[2]\n",
    "\n",
    "    if \"Puntuacion\" not in df.columns:\n",
    "        df[\"Puntuacion\"] = \"\"\n",
    "\n",
    "    total_prompt = total_completion = 0\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        title = str(row.get(col_titular, \"\") or \"\")\n",
    "        summ  = str(row.get(col_resumen, \"\") or \"\")\n",
    "        val, usage = score_with_openai(title, summ)\n",
    "        df.at[i, \"Puntuacion\"] = val\n",
    "        total_prompt     += usage.get(\"prompt_tokens\", 0)\n",
    "        total_completion += usage.get(\"completion_tokens\", 0)\n",
    "\n",
    "        if (i + 1) % 25 == 0:\n",
    "            print(f\"Progreso real: {i+1}/{len(df)} | tokens prompt={total_prompt} completion={total_completion}\")\n",
    "\n",
    "    # Guarda (igual al original + columna nueva)\n",
    "    df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Guardado: {OUTPUT_FILE.resolve()}\")\n",
    "    print(f\"Tokens usados: prompt={total_prompt}, completion={total_completion}, total={total_prompt+total_completion}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31b6893",
   "metadata": {},
   "source": [
    "Ahora hago la media de cada dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2489f2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Users\\gabri\\OneDrive\\Desktop\\Desktop\\UNIR\\TFM\\Codigo\\NOTICIAS_UNIFICADAS_5_ANIOS - V3 - puntuado - diario_media.csv  |  Filas: 436\n",
      "\n",
      "Comprobación (esperado 3, 7, 4, 2.4, 3.286):\n",
      "Fecha\n",
      "2025-07-28    1.667\n",
      "2025-07-29    7.000\n",
      "2025-07-30    3.000\n",
      "2025-07-31    5.000\n",
      "2025-08-01    2.688\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Media diaria de 'Puntuacion' alineada a días de mercado (ET, cierre→cierre 16:00).\n",
    "# Entrada: NOTICIAS_UNIFICADAS_5_ANIOS - V3 - puntuado.csv\n",
    "# Salida:  NOTICIAS_UNIFICADAS_5_ANIOS - V3 - puntuado - diario_media.csv\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import timedelta, date\n",
    "\n",
    "INPUT_FILE  = Path(\"NOTICIAS_UNIFICADAS_5_ANIOS - V3 - puntuado.csv\")\n",
    "OUTPUT_FILE = INPUT_FILE.with_name(f\"{INPUT_FILE.stem} - diario_media.csv\")\n",
    "\n",
    "ET_TZ = \"America/New_York\"\n",
    "CLOSE_H, CLOSE_M = 16, 0  \n",
    "\n",
    "def read_csv_any(p: Path) -> pd.DataFrame:\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"cp1252\", \"latin-1\"):\n",
    "        try:\n",
    "            return pd.read_csv(p, encoding=enc)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return pd.read_csv(p, engine=\"python\")\n",
    "\n",
    "def push_to_next_weekday(d: date) -> date:\n",
    "    while d.weekday() >= 5:  # 5=sábado, 6=domingo\n",
    "        d = d + timedelta(days=1)\n",
    "    return d\n",
    "\n",
    "def main():\n",
    "    df = read_csv_any(INPUT_FILE).copy()\n",
    "\n",
    "    # 1) Parseo de fecha-hora. El fichero viene en ET ya (naive), formato dd-mm-yy HH:MM.\n",
    "    dt1 = pd.to_datetime(df[\"Fecha\"], format=\"%d-%m-%y %H:%M\", errors=\"coerce\")\n",
    "    dt2 = pd.to_datetime(df.loc[dt1.isna(), \"Fecha\"], errors=\"coerce\", dayfirst=True)  # fallback\n",
    "    dt  = dt1.fillna(dt2)\n",
    "    # Localizar a ET (no convertir: las horas ya son ET)\n",
    "    dt  = dt.dt.tz_localize(ET_TZ, nonexistent=\"shift_forward\", ambiguous=\"infer\")\n",
    "\n",
    "    # 2) Puntuación válida\n",
    "    df[\"Puntuacion\"] = pd.to_numeric(df[\"Puntuacion\"], errors=\"coerce\")\n",
    "    m = dt.notna() & df[\"Puntuacion\"].notna()\n",
    "    df, dt = df.loc[m].copy(), dt.loc[m]\n",
    "\n",
    "    # 3) Día de mercado (cierre→cierre)\n",
    "    day   = dt.dt.floor(\"D\")\n",
    "    close = day + pd.to_timedelta(f\"{CLOSE_H:02d}:{CLOSE_M:02d}:00\")\n",
    "    after = dt >= close\n",
    "\n",
    "    market_day = day.dt.date\n",
    "    market_day = market_day.where(~after, market_day + pd.to_timedelta(1, unit=\"D\"))\n",
    "    market_day = market_day.apply(lambda d: push_to_next_weekday(pd.to_datetime(d).date()))\n",
    "    df[\"Market_Day\"] = pd.to_datetime(market_day)\n",
    "\n",
    "    # 4) Agregación\n",
    "    out = (\n",
    "        df.groupby(\"Market_Day\", as_index=False)[\"Puntuacion\"]\n",
    "          .mean()\n",
    "          .rename(columns={\"Market_Day\":\"Fecha\", \"Puntuacion\":\"Media_Puntuacion\"})\n",
    "          .sort_values(\"Fecha\")\n",
    "          .reset_index(drop=True)\n",
    "    )\n",
    "    out[\"Fecha\"] = pd.to_datetime(out[\"Fecha\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "    out[\"Media_Puntuacion\"] = out[\"Media_Puntuacion\"].round(3)\n",
    "    out[[\"Fecha\",\"Media_Puntuacion\"]].to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Guardado: {OUTPUT_FILE.resolve()}  |  Filas: {len(out)}\")\n",
    "\n",
    "    # 5) Chequeo rápido\n",
    "    chk = out.set_index(\"Fecha\").loc[[\"2025-07-28\",\"2025-07-29\",\"2025-07-30\",\"2025-07-31\",\"2025-08-01\"], \"Media_Puntuacion\"]\n",
    "    print(\"\\nComprobación (esperado 3, 7, 4, 2.4, 3.286):\")\n",
    "    print(chk.to_string())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aec361",
   "metadata": {},
   "source": [
    "Añado la columna de sentimiento a mi dataset grande rellenando con 0 las q no tienen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aedb0f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK -> AMZN_features_10y_basicos_plus_options_with_news.csv | filas = 2315 | nueva columna: 'news_sent_mean'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_40324\\343269838.py:33: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  d1 = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_40324\\343269838.py:34: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  d2 = pd.to_datetime(s, errors=\"coerce\", dayfirst=True, infer_datetime_format=True)\n",
      "C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_40324\\343269838.py:33: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  d1 = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
      "C:\\Users\\gabri\\AppData\\Local\\Temp\\ipykernel_40324\\343269838.py:34: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  d2 = pd.to_datetime(s, errors=\"coerce\", dayfirst=True, infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "# Añadir columna de sentimiento diario SIN CAMBIAR NADA del CSV original\n",
    "# - Lee features como TEXTO para preservar formato (fechas, decimales, etc.)\n",
    "# - Une por fecha usando una clave interna (sin modificar el CSV)\n",
    "# - Agrega 'news_sent_mean' al final; días sin noticia -> 0\n",
    "# Requisitos: pandas\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "FEAT_CSV = Path(\"AMZN_features_10y_basicos_plus_options.csv\")\n",
    "NEWS_CSV = Path(\"NOTICIAS_UNIFICADAS_5_ANIOS - V3 - puntuado - diario_media.csv\")\n",
    "OUT_CSV  = Path(\"AMZN_features_10y_basicos_plus_options_with_news.csv\")\n",
    "\n",
    "# --- Helpers ---\n",
    "def pick_date_col(df):\n",
    "    # Busca una columna de fecha por nombre típico; si no, prueba por parseo\n",
    "    cand_names = {\"date\",\"fecha\",\"datetime\",\"day\",\"dt\",\"time\",\"timestamp\"}\n",
    "    for c in df.columns:\n",
    "        if c.strip().lower() in cand_names:\n",
    "            return c\n",
    "    # si no, la que mejor se convierta a fecha\n",
    "    best, best_ok = None, 0\n",
    "    for c in df.columns:\n",
    "        ok = pd.to_datetime(df[c], errors=\"coerce\", infer_datetime_format=True).notna().mean()\n",
    "        if ok > best_ok:\n",
    "            best_ok, best = ok, c\n",
    "    if best is None or best_ok < 0.5:\n",
    "        raise ValueError(\"No pude identificar la columna de fecha.\")\n",
    "    return best\n",
    "\n",
    "def parse_dates_best(s):\n",
    "    # Intenta dos interpretaciones y elige la que tenga más válidos\n",
    "    d1 = pd.to_datetime(s, errors=\"coerce\", infer_datetime_format=True)\n",
    "    d2 = pd.to_datetime(s, errors=\"coerce\", dayfirst=True, infer_datetime_format=True)\n",
    "    d = d1 if d1.notna().sum() >= d2.notna().sum() else d2\n",
    "    return d.dt.normalize()\n",
    "\n",
    "# --- 1) Leer features COMO TEXTO (preserva formato exacto) ---\n",
    "feat_txt = pd.read_csv(FEAT_CSV, dtype=str, keep_default_na=False)  # no convierte NaN a float\n",
    "date_col_feat = pick_date_col(feat_txt)\n",
    "\n",
    "# Clave interna de fechas para alinear (no se guarda en el CSV)\n",
    "feat_key = parse_dates_best(feat_txt[date_col_feat])\n",
    "\n",
    "# --- 2) Leer noticias (numérico normal) y preparar media diaria ---\n",
    "news = pd.read_csv(NEWS_CSV)\n",
    "date_col_news = pick_date_col(news)\n",
    "news[date_col_news] = parse_dates_best(news[date_col_news])\n",
    "\n",
    "SENT_COL = \"Media_Puntuacion\"\n",
    "if SENT_COL not in news.columns:\n",
    "    raise ValueError(f\"No se encontró la columna {SENT_COL} en el CSV de noticias.\")\n",
    "\n",
    "news_daily = (\n",
    "    news.dropna(subset=[date_col_news])[ [date_col_news, SENT_COL] ]\n",
    "        .groupby(date_col_news, as_index=True)[SENT_COL]\n",
    "        .mean()\n",
    ")\n",
    "\n",
    "# --- 3) Generar la nueva columna alineada a las fechas del CSV de features ---\n",
    "new_col = feat_key.map(news_daily).fillna(0.0)  # 0 si no hay noticia ese día\n",
    "\n",
    "# Mantener el CSV idéntico y añadir columna al final\n",
    "feat_out = feat_txt.copy()\n",
    "feat_out[\"news_sent_mean\"] = new_col  # solo se añade esta columna\n",
    "\n",
    "# --- 4) Guardar (sin índice, mismo delimitador) ---\n",
    "feat_out.to_csv(OUT_CSV, index=False)\n",
    "print(f\"OK -> {OUT_CSV} | filas = {len(feat_out)} | nueva columna: 'news_sent_mean'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a50be44",
   "metadata": {},
   "source": [
    "Añado features derivadas de las noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34c44453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === Config ===\n",
    "FILE_IN  = \"AMZN_features_10y_basicos_plus_options_with_news.csv\"\n",
    "FILE_OUT = \"AMZN_features_10y_basicos_plus_options_with_news_feats.csv\"\n",
    "NEWS_COL = \"news_sent_mean\"\n",
    "\n",
    "# Ventanas por modelo\n",
    "WINDOWS = {\n",
    "    \"mc_1d\": 3,    # muy corto\n",
    "    \"c_21d\": 21,   # corto\n",
    "    \"m_252d\": 63,  # medio\n",
    "    \"l_504d\": 126  # largo\n",
    "}\n",
    "\n",
    "# === Carga (NO ordenar, NO tocar nada) ===\n",
    "df = pd.read_csv(FILE_IN)\n",
    "\n",
    "# Serie de trabajo (NO modifica la columna original en df)\n",
    "s = pd.to_numeric(df[NEWS_COL], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "# === Añadir SOLO las nuevas columnas al final ===\n",
    "for tag, w in WINDOWS.items():\n",
    "    df[f\"news_mean_w{w}\"] = s.rolling(window=w, min_periods=w).mean()\n",
    "    df[f\"news_std_w{w}\"]  = s.rolling(window=w, min_periods=w).std(ddof=0)\n",
    "\n",
    "# === Guardar (mismo orden de filas y columnas previas intactas) ===\n",
    "df.to_csv(FILE_OUT, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
